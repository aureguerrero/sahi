{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Detectron2_sahi.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNYaiLUZx6P8FyeipNwuQuD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aureguerrero/sahi/blob/main/Detectron2_sahi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LW7vytJ5S3qf"
      },
      "outputs": [],
      "source": [
        "!pip install -U git+https://github.com/aureguerrero/sahi.git\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cpu/torch1.10/index.html # for Detectron2-cpu\n",
        "#!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.10/index.html # for Detectron2-cuda11.1\n",
        "\n",
        "exit(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WACWXxs-YMZy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.getcwd()\n",
        "# will be used for detectron2 fasterrcnn model zoo name\n",
        "from sahi.utils.detectron2 import Detectron2TestConstants\n",
        "\n",
        "# import required functions, classes\n",
        "from sahi.model import Detectron2DetectionModel\n",
        "from sahi.predict import get_sliced_prediction, predict, get_prediction\n",
        "from sahi.utils.file import download_from_url\n",
        "from sahi.utils.cv import read_image\n",
        "from IPython.display import Image\n",
        "import numpy as np\n",
        "!pip install rasterio\n",
        "!pip install affine\n",
        "!pip install pyproj\n",
        "!pip install pygeoj\n",
        "!pip install geopandas\n",
        "import gdal\n",
        "import rasterio\n",
        "from affine import Affine\n",
        "from pyproj import Proj, transform\n",
        "import pygeoj\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import shutil\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "# set detectron2 fasterrcnn model zoo name\n",
        "model_path = Detectron2TestConstants.FASTERCNN_MODEL_ZOO_NAME\n",
        "\n",
        "# download test images into demo_data folder\n",
        "\n",
        "\n",
        "\n",
        "detection_model = Detectron2DetectionModel(\n",
        "    model_path=model_path,\n",
        "    config_path=model_path,\n",
        "    confidence_threshold=0.7,\n",
        "    image_size=640,\n",
        "    device=\"cpu\", # or 'cuda:0'\n",
        "    category_mapping={'0':'planta'},\n",
        ")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/Shareddrives/Conteo de plantas/SEMANTICA/pesos\n",
        "\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "cfg = get_cfg()\n",
        "#cfg.merge_from_file(os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\"))\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"planta_train\",)\n",
        "cfg.DATASETS.TEST = ()#(\"planta_val\",)\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = (os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\"))\n",
        "#cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
        "cfg.SOLVER.MAX_ITER = 1000    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n",
        "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
        "cfg.MODEL.DEVICE='cpu'\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ballon). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n",
        "# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n",
        "\n",
        "# Inference should use the config with parameters that are used in training\n",
        "# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.6   # set a custom testing threshold\n",
        "predictor = DefaultPredictor(cfg)\n",
        "detection_model.model=predictor\n",
        "%cd /\n",
        "%cd content/\n",
        "%mkdir images/\n",
        "%mkdir resultado/\n",
        "\n",
        "\n",
        "def rotate_bound(image, angle):\n",
        "    # grab the dimensions of the image and then determine the\n",
        "    # center\n",
        "    (h, w) = image.shape[:2]\n",
        "    (cX, cY) = (w // 2, h // 2)\n",
        "\n",
        "    # grab the rotation matrix (applying the negative of the\n",
        "    # angle to rotate clockwise), then grab the sine and cosine\n",
        "    # (i.e., the rotation components of the matrix)\n",
        "    M = cv2.getRotationMatrix2D((cX, cY), -angle, 1.0)\n",
        "    cos = np.abs(M[0, 0])\n",
        "    sin = np.abs(M[0, 1])\n",
        "\n",
        "    # compute the new bounding dimensions of the image\n",
        "    nW = int((h * sin) + (w * cos))\n",
        "    nH = int((h * cos) + (w * sin))\n",
        "\n",
        "    # adjust the rotation matrix to take into account translation\n",
        "    M[0, 2] += (nW / 2) - cX\n",
        "    M[1, 2] += (nH / 2) - cY\n",
        "\n",
        "    # perform the actual rotation and return the image\n",
        "    return cv2.warpAffine(image, M, (nW, nH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qodRHC038ixL"
      },
      "outputs": [],
      "source": [
        "ancho_ventana=512\n",
        "alto_ventana=512\n",
        "solapamiento_ancho=0.2\n",
        "solapamiento_alto=0.2\n",
        "prob_de_coinc_pospro=0.6\n",
        "detection_model.confidence_threshold=0.7 # nivel de confianza para que muestre un objeto como predicciÃ³n\n",
        "centroides=1 # si es None no imprime en imagen los centroides si es 1 imprime centroides"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugbvLi2anVoA"
      },
      "outputs": [],
      "source": [
        "for i in os.listdir('images'):\n",
        "  fname = '/content/images/'+i\n",
        "  result= get_sliced_prediction(\n",
        "    fname,\n",
        "    detection_model,\n",
        "    slice_height = alto_ventana,\n",
        "    slice_width = ancho_ventana,\n",
        "    overlap_height_ratio = solapamiento_alto,\n",
        "    overlap_width_ratio = solapamiento_ancho,\n",
        "    postprocess_match_threshold= prob_de_coinc_pospro,\n",
        "    )\n",
        "  result.export_visuals(export_dir='resultado/',export_file='predict_'+i[:-4],centro=centroides)\n",
        "\n",
        "  with open('resultado/predict_'+i[:-4],'wb') as planta:\n",
        "    pickle.dump(result,planta,protocol=pickle.HIGHEST_PROTOCOL) \n",
        "\n",
        "# Para abrir el archivo con resultados\n",
        "  #  with open('resultado/predict_'+i[:-4],'rb') as planta:\n",
        "  #   result=pickle.load(planta)\n",
        "\n",
        "  # Ver los centroides\n",
        "  # result.centroides()\n",
        "\n",
        "  if i[-3:]=='tif' or i[-4:]=='tiff':\n",
        "    with rasterio.open(fname) as r:\n",
        "      T0 = r.transform  # upper-left pixel corner affine transform\n",
        "      p1 = Proj(r.crs)\n",
        "      A = r.read()\n",
        "    datos={\"type\": \"FeatureCollection\",\"name\": i+'_vectorizacion',\"crs\": { \"type\": \"name\", \"properties\": { \"name\": \"urn:ogc:def:crs:\"+r.crs.to_string() } }, \"features\":[]}\n",
        "    centros={\"type\": \"FeatureCollection\",\"name\": i+'_vectorizacion',\"crs\": { \"type\": \"name\", \"properties\": { \"name\": \"urn:ogc:def:crs:\"+r.crs.to_string() } }, \"features\":[]}\n",
        "\n",
        "    id=0\n",
        "    for obj,c in zip(result.object_prediction_list,result.centroides()):\n",
        "      img=np.uint8(255*obj.mask.bool_mask)\n",
        "      contours, hier = cv2.findContours(rotate_bound(cv2.flip(rotate_bound(img, 90),0),180), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "      cont=np.array(contours).squeeze()\n",
        "      if len(np.shape(cont))==1:\n",
        "        planta=[]\n",
        "        area=0\n",
        "        for arreglo in cont:\n",
        "          if len(arreglo)>2:\n",
        "            arreglo=arreglo.squeeze()\n",
        "            area=area+cv2.contourArea(arreglo)*abs(T0[0]*T0[4])\n",
        "            arreglo=np.append(arreglo,[arreglo[0,:]],axis=0)\n",
        "            arreglo=(arreglo+np.array([obj.bbox.miny,obj.bbox.minx]))*np.array([T0[4],T0[0]])+np.array([T0[5],T0[2]])\n",
        "            arreglo=[[arreglo[i][1],arreglo[i][0]] for i in range(np.shape(arreglo)[0])]\n",
        "            planta.append(arreglo)\n",
        "        datos['features'].append({ \"type\": \"Feature\", \"properties\": { \"CLASS_NAME\": \"planta\", \"AREA\": area }, \"geometry\": { \"type\": \"MultiPolygon\", \"coordinates\":[planta]}})\n",
        "\n",
        "      else:\n",
        "        area=cv2.contourArea(cont)*abs(T0[0]*T0[4])\n",
        "        cont=np.append(cont,[cont[0,:]],axis=0)\n",
        "        cont=(cont+np.array([obj.bbox.miny,obj.bbox.minx]))*np.array([T0[4],T0[0]])+np.array([T0[5],T0[2]])\n",
        "        cont=[[cont[i][1],cont[i][0]] for i in range(np.shape(cont)[0])]\n",
        "        datos['features'].append({ \"type\": \"Feature\", \"properties\": { \"CLASS_NAME\": \"planta\", \"AREA\": area }, \"geometry\": { \"type\": \"MultiPolygon\", \"coordinates\":[[cont]]}})\n",
        "      centros['features'].append({ \"type\": \"Feature\", \"properties\": { \"CLASS_NAME\": \"planta\", \"AREA\": area, \"ID\" :id}, \"geometry\": { \"type\": \"Point\", \"coordinates\":[c[0]*T0[4]+T0[5],c[1]*T0[0]+T0[2]]}})\n",
        "      id=id+1\n",
        "    with open('/content/resultado/'+i[:-4]+'_vectorial.geojson', 'w') as fp:\n",
        "      json.dump(datos, fp)\n",
        "    with open('/content/resultado/'+i[:-4]+'_centroides.geojson', 'w') as fp:\n",
        "      json.dump(centros, fp)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBkbdrActH0b"
      },
      "outputs": [],
      "source": [
        "!zip -r resultados.zip resultado/"
      ]
    }
  ]
}
